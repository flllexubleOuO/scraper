#!/bin/bash
# AWS EC2 éƒ¨ç½²è„šæœ¬

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "ğŸš€ Starting deployment to AWS EC2..."

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# æ£€æŸ¥æ˜¯å¦ä¸ºrootç”¨æˆ·
if [ "$EUID" -eq 0 ]; then 
    echo -e "${RED}è¯·ä¸è¦ä½¿ç”¨rootç”¨æˆ·è¿è¡Œæ­¤è„šæœ¬${NC}"
    exit 1
fi

# æ›´æ–°ç³»ç»Ÿ
echo -e "${GREEN}ğŸ“¦ æ›´æ–°ç³»ç»ŸåŒ…...${NC}"
sudo apt-get update -y
sudo apt-get upgrade -y

# å®‰è£…Python 3å’Œpip
echo -e "${GREEN}ğŸ å®‰è£…Python 3...${NC}"
sudo apt-get install -y python3 python3-pip python3-venv

# å®‰è£…Gitï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
echo -e "${GREEN}ğŸ“¥ å®‰è£…Git...${NC}"
sudo apt-get install -y git

# å®‰è£…Chromeå’ŒChromeDriverï¼ˆç”¨äºSeleniumï¼‰
echo -e "${GREEN}ğŸŒ å®‰è£…Chromeå’ŒChromeDriver...${NC}"
sudo apt-get install -y wget unzip
wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo apt-get install -y ./google-chrome-stable_current_amd64.deb
rm google-chrome-stable_current_amd64.deb

# å®‰è£…ChromeDriver
CHROME_VERSION=$(google-chrome --version | awk '{print $3}' | cut -d. -f1)
echo "Chrome version: $CHROME_VERSION"
wget -q "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_VERSION}" -O /tmp/chromedriver_version
CHROMEDRIVER_VERSION=$(cat /tmp/chromedriver_version)
wget -q "https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip"
unzip -q chromedriver_linux64.zip
sudo mv chromedriver /usr/local/bin/
sudo chmod +x /usr/local/bin/chromedriver
rm chromedriver_linux64.zip

# å…‹éš†æˆ–æ›´æ–°ä»£ç 
APP_DIR="$HOME/scraper"
if [ -d "$APP_DIR" ]; then
    echo -e "${GREEN}ğŸ“‚ æ›´æ–°ä»£ç ...${NC}"
    cd "$APP_DIR"
    git pull origin main
else
    echo -e "${GREEN}ğŸ“‚ å…‹éš†ä»£ç ä»“åº“...${NC}"
    cd "$HOME"
    git clone https://github.com/flllexubleOuO/scraper.git
    cd scraper
fi

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
echo -e "${GREEN}ğŸ”§ åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ...${NC}"
python3 -m venv venv
source venv/bin/activate

# å®‰è£…ä¾èµ–
echo -e "${GREEN}ğŸ“¦ å®‰è£…Pythonä¾èµ–...${NC}"
pip install --upgrade pip
pip install -r requirements.txt

# åˆ›å»ºç¯å¢ƒå˜é‡æ–‡ä»¶
if [ ! -f .env ]; then
    echo -e "${YELLOW}âš ï¸  åˆ›å»º.envæ–‡ä»¶...${NC}"
    cat > .env << EOF
# OpenAI API Key
OPENAI_API_KEY=your_openai_api_key_here

# Database
DATABASE_PATH=job_scraper.db

# Flask
FLASK_SECRET_KEY=$(openssl rand -hex 32)
FLASK_PORT=8080
EOF
    echo -e "${YELLOW}âš ï¸  è¯·ç¼–è¾‘ .env æ–‡ä»¶å¹¶å¡«å…¥ä½ çš„OpenAI API Key${NC}"
    echo -e "${YELLOW}   nano ~/.env${NC}"
fi

# åˆ›å»ºsystemdæœåŠ¡æ–‡ä»¶ï¼ˆWebåº”ç”¨ï¼‰
echo -e "${GREEN}ğŸ”§ é…ç½®systemdæœåŠ¡...${NC}"
sudo tee /etc/systemd/system/scraper-web.service > /dev/null << EOF
[Unit]
Description=NZ IT Job Market Analyzer Web App
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=$APP_DIR
Environment="PATH=$APP_DIR/venv/bin"
EnvironmentFile=$APP_DIR/.env
ExecStart=$APP_DIR/venv/bin/python $APP_DIR/simple_app.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

# åˆ›å»ºsystemdæœåŠ¡æ–‡ä»¶ï¼ˆå®šæ—¶ä»»åŠ¡ï¼‰
sudo tee /etc/systemd/system/scraper-scheduler.service > /dev/null << EOF
[Unit]
Description=NZ IT Job Market Analyzer Scheduler
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=$APP_DIR
Environment="PATH=$APP_DIR/venv/bin"
EnvironmentFile=$APP_DIR/.env
ExecStart=$APP_DIR/venv/bin/python $APP_DIR/scheduler_daemon.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

# é‡è½½systemd
sudo systemctl daemon-reload

# å¯åŠ¨æœåŠ¡
echo -e "${GREEN}ğŸš€ å¯åŠ¨æœåŠ¡...${NC}"
sudo systemctl enable scraper-web
sudo systemctl start scraper-web
sudo systemctl enable scraper-scheduler
sudo systemctl start scraper-scheduler

# é…ç½®é˜²ç«å¢™ï¼ˆå¦‚æœä½¿ç”¨UFWï¼‰
if command -v ufw &> /dev/null; then
    echo -e "${GREEN}ğŸ”’ é…ç½®é˜²ç«å¢™...${NC}"
    sudo ufw allow 8080/tcp
    sudo ufw allow 22/tcp  # SSH
fi

# æ˜¾ç¤ºçŠ¶æ€
echo ""
echo -e "${GREEN}âœ… éƒ¨ç½²å®Œæˆï¼${NC}"
echo ""
echo "ğŸ“Š æœåŠ¡çŠ¶æ€ï¼š"
sudo systemctl status scraper-web --no-pager
echo ""
sudo systemctl status scraper-scheduler --no-pager
echo ""
echo "ğŸŒ è®¿é—®åœ°å€ï¼š"
echo "   http://$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4):8080"
echo ""
echo "ğŸ“ æœ‰ç”¨çš„å‘½ä»¤ï¼š"
echo "   æŸ¥çœ‹Webæ—¥å¿—:       sudo journalctl -u scraper-web -f"
echo "   æŸ¥çœ‹Scheduleræ—¥å¿—: sudo journalctl -u scraper-scheduler -f"
echo "   é‡å¯WebæœåŠ¡:       sudo systemctl restart scraper-web"
echo "   é‡å¯Scheduler:     sudo systemctl restart scraper-scheduler"
echo "   åœæ­¢æœåŠ¡:          sudo systemctl stop scraper-web scraper-scheduler"
echo ""
echo -e "${YELLOW}âš ï¸  åˆ«å¿˜äº†åœ¨AWSå®‰å…¨ç»„ä¸­å¼€æ”¾8080ç«¯å£ï¼${NC}"

