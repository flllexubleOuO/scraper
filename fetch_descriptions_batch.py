#!/usr/bin/env python3
"""
æ‰¹é‡æŠ“å–èŒä½æè¿° - é’ˆå¯¹ä½å†…å­˜ç¯å¢ƒä¼˜åŒ–
æ¯æŠ“å–Nä¸ªèŒä½å°±é‡å¯æµè§ˆå™¨ï¼Œé¿å…å†…å­˜æ³„æ¼
"""

import sqlite3
import logging
import time
from typing import List, Tuple

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_jobs_without_description(db_path: str, limit: int = None) -> List[Tuple]:
    """è·å–æ²¡æœ‰æè¿°çš„èŒä½"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    query = """
        SELECT id, job_id, url, title, company
        FROM jobs 
        WHERE (description IS NULL OR description = '') 
        AND url IS NOT NULL
        AND source = 'seek'
        ORDER BY id DESC
    """
    
    if limit:
        query += f" LIMIT {limit}"
    
    jobs = cursor.fetchall()
    conn.close()
    return jobs

def fetch_description_batch(db_path: str, batch_size: int = 5, max_total: int = None, delay: float = 3.0):
    """åˆ†æ‰¹æŠ“å–èŒä½æè¿°ï¼ˆä½å†…å­˜ä¼˜åŒ–ï¼‰"""
    
    # è·å–éœ€è¦æŠ“å–çš„èŒä½
    jobs = get_jobs_without_description(db_path, max_total)
    
    if not jobs:
        logger.info("âœ… All jobs already have descriptions!")
        return
    
    logger.info(f"ğŸ“Š Found {len(jobs)} jobs without descriptions")
    logger.info(f"ğŸ“¦ Processing in batches of {batch_size}")
    
    total_success = 0
    total_failed = 0
    
    # åˆ†æ‰¹å¤„ç†
    for batch_start in range(0, len(jobs), batch_size):
        batch_end = min(batch_start + batch_size, len(jobs))
        batch = jobs[batch_start:batch_end]
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ”„ Processing batch {batch_start//batch_size + 1}/{(len(jobs)-1)//batch_size + 1}")
        logger.info(f"   Jobs {batch_start+1}-{batch_end} of {len(jobs)}")
        logger.info(f"{'='*60}")
        
        # åˆå§‹åŒ–çˆ¬è™«ï¼ˆæ¯æ‰¹æ¬¡æ–°å»ºï¼‰
        from scrapers.seek_scraper import SeekScraper
        scraper = SeekScraper()
        
        try:
            # å¤„ç†æœ¬æ‰¹æ¬¡
            for job_id, job_external_id, url, title, company in batch:
                try:
                    logger.info(f"ğŸ“„ Fetching: {title} at {company}")
                    
                    # æŠ“å–æè¿°
                    description = scraper.fetch_job_description(url)
                    
                    if description:
                        # ä¿å­˜åˆ°æ•°æ®åº“
                        conn = sqlite3.connect(db_path)
                        cursor = conn.cursor()
                        cursor.execute(
                            "UPDATE jobs SET description = ? WHERE id = ?",
                            (description, job_id)
                        )
                        conn.commit()
                        conn.close()
                        
                        total_success += 1
                        logger.info(f"   âœ… Success (length: {len(description)})")
                    else:
                        total_failed += 1
                        logger.warning(f"   âš ï¸ No description found")
                    
                    # å»¶è¿Ÿï¼Œå‡å°‘å†…å­˜å‹åŠ›
                    time.sleep(delay)
                    
                except Exception as e:
                    total_failed += 1
                    logger.error(f"   âŒ Error: {e}")
                    continue
            
        finally:
            # å…³é—­æµè§ˆå™¨
            try:
                scraper.close_driver()
                logger.info(f"ğŸ”Œ Browser closed for batch {batch_start//batch_size + 1}")
            except:
                pass
        
        # æ‰¹æ¬¡é—´ä¼‘æ¯ï¼Œè®©ç³»ç»Ÿå……åˆ†æ¢å¤å†…å­˜
        if batch_end < len(jobs):
            rest_time = 10
            logger.info(f"ğŸ˜´ Resting {rest_time} seconds before next batch (memory recovery)...")
            time.sleep(rest_time)
    
    # æœ€ç»ˆç»Ÿè®¡
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“Š Final Statistics")
    logger.info(f"{'='*60}")
    logger.info(f"âœ… Successful: {total_success}")
    logger.info(f"âŒ Failed: {total_failed}")
    logger.info(f"ğŸ“ˆ Success rate: {total_success/(total_success+total_failed)*100:.1f}%")
    logger.info(f"{'='*60}")


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Fetch job descriptions in batches (memory-optimized)')
    parser.add_argument('--db', default='job_scraper.db', help='Database path')
    parser.add_argument('--batch-size', type=int, default=5, help='Jobs per batch (default: 5 for low memory)')
    parser.add_argument('--max-total', type=int, default=None, help='Maximum total jobs to process')
    parser.add_argument('--delay', type=float, default=3.0, help='Delay between jobs in seconds (default: 3)')
    
    args = parser.parse_args()
    
    logger.info("ğŸš€ Starting batch description fetcher (LOW MEMORY MODE)")
    logger.info(f"   Database: {args.db}")
    logger.info(f"   Batch size: {args.batch_size} (smaller = less memory)")
    logger.info(f"   Delay per job: {args.delay}s (longer = safer)")
    logger.info(f"   Max total: {args.max_total or 'unlimited'}")
    logger.info(f"   â±ï¸  Estimated time: ~{(args.max_total or 500) * (args.delay + 2) / 60:.0f} minutes")
    
    fetch_description_batch(args.db, args.batch_size, args.max_total, args.delay)
    
    logger.info("\nğŸ‰ Batch processing completed!")

